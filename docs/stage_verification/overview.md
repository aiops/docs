# Stage Verification

`logsight.ai` supports the continuous verification of deployments, comparing tests, detecting test flakiness 
and other log verification tasks via the task of AI-powered `log compare`.

<div align=center>
    <img width="400"  src="/stage_verification/concept.png"/>
</div>


## Intelligent Verification

### Log Analysis

The development of reliable software typically involves the adoption of interactive debugging and log file analysis. 
While debugging tools became very sophisticated over the years, the same cannot be said about the tools available for log analysis.  

The benefit of a debugger is that it lets developers see all the "states" of a program during execution.
By state, we refer to a line of code executed with the data stored in variables.
Debuggers are used manually during code development by developers.

When testing software via, e.g., unit or integration testing, it is common to rely on another method: log file analysis.
With this method, states executed by a program are recorded in a log file which are later analysed by developers.
Since a program can take too many states, developers carefully select relevant/critical states which should be reported.
These states are often considered important or useful for program monitoring and/or locating faults.


### Manual vs. Automated Verification

During testing, and once the program completes its execution, developers manually scan log files to check if new critical states were recorded.
The task consists in comparing the log records of program version A and version B. It is a form of software A/B testing.
One current limitation of log file analysis is that it is still a manual task. 
This is especially true since log files are often very large and can have complex structure.

At logsight.ai, we looked into ways to automate log file analysis. 
Our solution works based on the important notion of state previously discussed.


### States

A state includes a level/severity (e.g., warning), and a textual description which is often parametrized with
the current values of variables, return values of function calls or any other state information.
   
    logging.warning('Unable to connect to database: %s:%s', ip, port)

At runtime, if reached, the state above will generate a log record similar to:

    2017-05-16 11:32:09 WARN Unable to connect to database: 192.168.0.1:9090

To find the states reached by an application using log records, we rely on `state mining`. 
This technique converts log records, which describe a state of an application, into a feature vector of variables/values. 

<div align=center>
<img width="600" src="/stage_verification/state_mining.png"/>
</div>

Continuous verification also uses semantic analysis and natural language understanding (NLU) to compare logs. 
Its AI-model was trained to understand the hidden and underlying latent semantics of words in logs messages.
This allows to compare logs and discover differences which are correlated with failures.


### A/B State Analysis

Our approach to verification compares two runs versions of the same application to identify critical differences in the log records generated.
This spotting-the-differences approach uses `state mining` identify the states reached by each version A and version B.
Based on the changes of state frequency, count, severity, and semantics, a deployment risk is calculated.  

The following table shows an example.

+ State ID 1 was generated by both versions A and B. Since state count change from A to B was low (+8%), the deployment risk is None.      
+ State ID 2 is similar to state ID 1. Since there is a moderate -34% drop of the number of states recorded in version B, the deployment risk is set as Low.  
+ State ID 3 is similar to state ID 2. But in this case, its level is warning (WARN) and it has been a +162% increase in version B. Thus, the deployment risk is Medium.
+ State ID 4 was added in version B. Since it is new, with a +12 count, and it has the level WARN, the deployment risk is set to High.
+ State ID 5 was removed in version B. Since an ERROR was remove from the application, the deployment risk is None.

| ID | Timestamp | Level | Log message                         | State                        |  A  |  B  | Change |  Risk  |
|:--:|:---------:|-------|-------------------------------------|------------------------------|:---:|:---:|:------:|:------:|
|  1 |    8h21   | INFO  | Customer id=111-222 data stored     | Customer id=$1 data stored   | Yes | Yes |   +8%  |  None  |
|  2 |    8h22   | INFO  | Processing request req=A12-345      | Processing request req=$1    | Yes | Yes |  -34%  |   Low  |
|  3 |    8h24   | WARN  | Retrying (#15) request req=A22-222  | Retrying ($1) request req=$2 | Yes | Yes |  +162% | Medium |
|  4 |    8h31   | WARN  | Cannot connect to: 192.168.0.1:9090 | Unable to connect to: $1:$2  |  No | Yes |   +12  |  High  |
|  5 |    8h35   | ERROR | Insufficient memory (64GB)          | Insufficient memory ($1)     | Yes |  No |   -7   |  None  |

Table. Example verifying version B with respect to version A    

The final risk is calculated by considering each individual risk. 
In this example, the deployment risk was considered to be High


### How is Deployment Risk Calculated?

The deployment risk is calculated using th following table. 
A risk score is assigned for each state occurring in version A and version B.  
For example, if version B has a new state labelled as error but with no semantic anomaly, the state receives a risk score of 50 points. 

| State     | Error Level | Semantic Anomaly | Frequency Change | Risk |
|-----------|:-----------:|:----------------:|:----------------:|:----:|
| Added     |     Yes     |        Yes       |                  |  80  |
|           |     Yes     |        No        |                  |  50  |
|           |      No     |        Yes       |                  |  50  |
|           |      No     |        No        |                  |   0  |
| Deleted   |             |                  |                  |   0  |
| Recurring |     Yes     |        No        |        No        |  30  |
|           |      No     |        Yes       |        No        |  30  |
|           |      No     |        No        |        No        |   0  |
|           |     Yes     |        No        |        Yes       |  50  |
|           |      No     |        Yes       |        Yes       |  50  |
|           |      No     |        No        |        Yes       |  10  |

Table. Risk score assigned to individual states   

For each state, a risk score is calculated. Afterwards, the deployment risk is calculated using the following formulae. 

+ Deployment Risk = max(risk of all states) + min(average risk of top-k states, 100 - max risk)

If the Deployment Risk is greater thant the deployment risk threshold (e.g., 80), the verification gate stops the deployment. 


<!---
One benefit of this approach is that it can easily be used by less-experienced troubleshooting developers or testers.
-->

> [!NOTE]
> With logsight.ai UI, version A of an application is called `Baseline` and version B is called `Candidate`
