# Stage Verification

`logsight.ai` supports the continuous verification of deployments, comparing tests, and other log verification tasks.

<div align=center>
    <img width="400"  src="/stage_verification/concept.png"/>
</div>


## Introduction

### Log Analysis

The development of reliable software typically involves the use of interactive debugging tools and the analysis of log files. 
While debugging tools became very sophisticated over the years, the same cannot be said about the solutions available for log analysis.  

The benefit of a debugger is that it lets developers see all the "states" of a program during its execution.
By state, we refer to a line of code executed and the data stored in variables.
Developers manually use debuggers during code development.

When testing software via, e.g., unit or integration testing, it is common to rely on another method: log file analysis.
With this method, the states executed by a program are recorded in a log file which is later analysed by developers.
Since a program can generate a large quantity of states, developers carefully select the states considered important or useful for program monitoring and/or locating faults.


### Manual vs. Automated Verification

During testing, and once the program completes its execution, developers manually scan log files to check if error states were recorded.
The task consists in comparing the log records of version B of a program with the logs of version A.
It is a form of software A/B testing.

One current limitation of log file analysis is that it is still a manual task. 
This limitation is aggravated when log files are large and with a complex structure.

At logsight.ai, we automate log file analysis. 
Our solution works based on the important notion of state.


## States

### State Mining 

In simple terms, a state is a logging statement used to record valuable runtime information about applications.
A state includes a level or severity (e.g., warning), and a textual description which is often parametrized with
the current values of variables and context information (e.g., thread id, request id).
   
    logging.warning('Unable to connect to database: %s:%s', ip, port)

At runtime, if reached, the state above will generate a log record similar to:

    2017-05-16 11:32:09 WARN Unable to connect to database: 192.168.0.1:9090

To find the states generated/reached by an application using log records, we rely on `state mining`. 
This technique converts log file records into a feature vector of variables/values. 
For example, the following log record,

| Timestamp | Level | Log message                          | 
|:---------:|-------|--------------------------------------|
|    8h31   | INFO  | Cannot connect to: 192.168.0.1:9090  |

is abstracted using state mining to:

| Timestamp | State                    | Variables               | Level | 
|:---------:|--------------------------|-------------------------|:-----:|
|    8h31   | Cannot connect to: $1:$2 | $1=192.168.0.1, $2=9090 |  INFO |


### State Semantics

Continuous verification also uses semantic analysis and natural language understanding (NLU) to compare logs. 
Its AI-model was trained to understand the hidden and underlying latent semantics of words in logs messages.
This allows to compare logs and discover differences which are correlated with failures.

As an example, the state shown previously is extended to include semantic information.  

| Timestamp | State                      | Variables               |  Level  | Semantics |
|:---------:|----------------------------|-------------------------|:-------:|:---------:|
|    8h31   | Cannot connect to: $1:$2   | $1=192.168.0.1, $2=9090 |  INFO   |   FAULT   |


## Verification

Our approach to verification compares two runs (versions) of the same application to identify critical differences in the states generated.
This spotting-the-differences approach identifies the states reached by each version A and version B.
Based on the changes of state frequency, count, severity, and semantics, a `risk score` is calculated for each state.
Afterwards, a `deployment risk` is calculated based on all the individual risk scores.  


### Risk Score

The following table shows an example on how the risk score is calculated.

+ State ID 1 was generated by both versions A and B. Since state count change from A to B was low (+8%), the risk score is 0.      
+ State ID 2 is similar to state ID 1. Since there is a significant -94% drop of the number of INFO states recorded in version B, the risk is set as 10.  
+ State ID 3 is similar to state ID 2. But in this case, its level is warning (WARN) and it has been a +162% increase in version B. Thus, the risk is 50.
+ State ID 4 was added in version B. Since it is new, with a +12 count, and it has the level WARN, the risk score is set to 50.
+ State ID 5 was removed in version B. Since an ERROR state was remove from the application, the risk is 0.


| ID | Timestamp | Level | Semantics | Log message                         | State                        |  A  |  B  | Change | Risk Score |
|:--:|:---------:|-------|:---------:|-------------------------------------|------------------------------|:---:|:---:|:------:|:----------:|
|  1 |    8h21   | INFO  |    REPORT   | Customer id=111-222 data stored     | Customer id=$1 data stored   | Yes | Yes |   +8%  |      0     |
|  2 |    8h22   | INFO  |    REPORT   | Processing request req=A12-345      | Processing request req=$1    | Yes | Yes |  -94%  |     10     |
|  3 |    8h24   | WARN  |    REPORT   | Retrying (#15) request req=A22-222  | Retrying ($1) request req=$2 | Yes | Yes |  +162% |     50     |
|  4 |    8h31   | INFO  |   FAULT   | Cannot connect to: 192.168.0.1:9090 | Cannot connect to: $1:$2     |  No | Yes |   +12  |     50     |
|  5 |    8h35   | ERROR |   FAULT   | Insufficient memory (64GB)          | Insufficient memory ($1)     | Yes |  No |   -7   |      0     |

Table 1. Example of risk score calculation    


The risk score is set using the following table. 
For example, if a version has a new Added state labelled with Level = ERROR, but with Semantics != FAULT, the state receives a risk score of 50 points. 

| State     | Level = ERROR | Semantics = FAULT | Change = High | Risk Score |
|-----------|:-------------:|:-----------------:|:-------------:|:----------:|
| Added     |      Yes      |        Yes        |               |     80     |
|           |      Yes      |        No         |               |     50     |
|           |      No       |        Yes        |               |     50     |
|           |      No       |        No         |               |      0     |
| Deleted   |               |                   |               |      0     |
| Recurring |      Yes      |        No         |      No       |     30     |
|           |      No       |        Yes        |      No       |     30     |
|           |      No       |        No         |      No       |      0     |
|           |      Yes      |        No         |      Yes      |     50     |
|           |      No       |        Yes        |      Yes      |     50     |
|           |      No       |        No         |      Yes      |     10     |

Table 2. Mapping between state characteristics and risk score     


### Deployment Risk

The deployment risk is calculated by considering each individual risk score. 
The following formulae is used: 

+ Deployment Risk = max(risk score of all states) + min(average risk score, 100 - max risk score)

If the deployment risk is greater thant the deployment risk threshold (e.g., 80), the verification gate stops the deployment. 

For example, the deployment risk of the states from Table 1 is:

+ max([0, 10, 50, 50, 0]) + min(average([0, 10, 50, 50, 0]), 100 - max([0, 10, 50, 50, 0])) = 50 + min(22, 100 - 50) = 72

<!---
One benefit of this approach is that it can easily be used by less-experienced troubleshooting developers or testers.
-->


> [!NOTE]
> With logsight.ai UI, version A of an application is called `Baseline` and version B is called `Candidate`
